apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: kube-prometheus-stack-alertmanagerconfig
  namespace: monitoring
  labels:
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/name: alertmanagerconfig
    alertmanagerConfig: slack
spec:
  #routes:
  route:
    groupBy: ['severity']
    receiver: slack_router
    groupWait: 0s
    groupInterval: 5m
    repeatInterval: 2h      
  receivers:
    - name: slack_router
      slackConfigs:
        - apiURL:
            name: alertmanager-secrets
            key: slack
          channel: '#alert'  
          sendResolved: true

          title: |-
            [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
            {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
              {{" "}}(
              {{- with .CommonLabels.Remove .GroupLabels.Names }}
                {{- range $index, $label := .SortedPairs -}}
                  {{ if $index }}, {{ end }}
                  {{- $label.Name }}="{{ $label.Value -}}"
                {{- end }}
              {{- end -}}
              )
            {{- end }}          
          text: >-
            {{ range .Alerts -}}
            *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
            *Description:* {{ .Annotations.description }}
            *Details:*
              {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
              {{ end }}
            {{ end }}    

 #   - name: pagerduty
#      pagerdutyConfigs:
#        - routingKey:
#           key: pagerDutyIntegrationV2
#           name: alertmanager-secrets
#          severity: 'critical'
          #severity: '{{ if .CommonLabels.severity }}{{ .CommonLabels.severity | toLower }}{{ else }}critical{{ end }}'
